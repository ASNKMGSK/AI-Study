{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "radio-therapist",
   "metadata": {},
   "source": [
    "# 12. CIFAR-10 이미지 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "relevant-pleasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /home/asnkmgsk/anaconda3/lib/python3.7/site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy in /home/asnkmgsk/anaconda3/lib/python3.7/site-packages (from imageio) (1.19.5)\n",
      "Requirement already satisfied: pillow in /home/asnkmgsk/anaconda3/lib/python3.7/site-packages (from imageio) (8.1.0)\n",
      "Requirement already satisfied: Pillow in /home/asnkmgsk/anaconda3/lib/python3.7/site-packages (8.1.0)\n"
     ]
    }
   ],
   "source": [
    "# 먼저 imageio, pillow 라이브러를 사용할 것이므로, 없으면 설치하자\n",
    "!pip install imageio\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pleased-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리고 다음과 같이 작업 환경을 구성한다   \n",
    "# $ mkdir -p ~/aiffel/dcgan_newimage/cifar10/generated_samples   \n",
    "# $ mkdir -p ~/aiffel/dcgan_newimage/cifar10/training_checkpoints   \n",
    "# $ mkdir -p ~/aiffel/dcgan_newimage/cifar10/training_history   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unlike-carnival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 2.2.0\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러를 import 해준다\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "import imageio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-effectiveness",
   "metadata": {},
   "source": [
    "## 12.1 데이터셋 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 데이터셋은 tf.keras 안에 있는 datasets에 포함되어 있어서, 아래와 같이 손쉽게 데이터셋을 구성할 수 있다\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "\n",
    "(train_x, _), (test_x, _) = cifar10.load_data()\n",
    "\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 픽셀 값을 확인해본다\n",
    "print(\"max pixel:\", train_x.max())\n",
    "print(\"min pixel:\", train_x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 픽셀을 -1, 1로 정규화시켜서 사용할 예정이므로, 중간값을 0으로 맞춰주기 위해 127.5를 뺀 후 127.5로 나눠준다\n",
    "train_x = (train_x - 127.5) / 127.5 # 이미지를 [-1, 1]로 정규화\n",
    "\n",
    "print(\"max pixel:\", train_x.max())\n",
    "print(\"min pixel:\", train_x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋의 shape를 확인해보자\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "\n",
    "for i in range(15):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    random_index = np.random.randint(1, 50000)\n",
    "    plt.imshow((train_x[random_index] + 1)/2.0)\n",
    "    plt.title(f'index: {random_index}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-modem",
   "metadata": {},
   "source": [
    "이렇게 정리된 데이터를 곧 모델에 넣어서 학습시켜야 하니, 편하게 사용할 수 있도록 텐서플로우의 Dataset을 이용해 준비해 놓도록 하겠다.   \n",
    "이를 이용하면 우리가 매번 모델에게 직접 섞어서 넣어주지 않아도 된다. 어떻게 사용하는지는 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE은 전체 데이터를 섞기 위해 51,000으로 설정한다. shuffle() 함수가 데이터셋을 잘 섞어서 모델에 넣어줄 것이다.\n",
    "BUFFER_SIZE = 51000\n",
    "BATCH_SIZE = 320\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_x).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-satin",
   "metadata": {},
   "source": [
    "tf.data.Dataset 모듈의 from_tensor_slices() 함수를 사용하면 리스트, 넘파이, 또는 텐서플로우의 텐서 자료형에서 데이터셋을 만들 수 있다.   \n",
    "위 코드는 train_x라는 넘파이 배열(numpy ndarray)형 자료를 섞고, 이를 배치 사이즈에 따라 나누도록 한다.   \n",
    "데이터가 잘 섞이게 하기 위해서는 버퍼 사이즈를 총 데이터 사이즈와 같거나 크게 설정하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-municipality",
   "metadata": {},
   "source": [
    "## 12.2 생성자 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성자 모델 정의\n",
    "def make_generator_model():\n",
    "\n",
    "    # Start\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # First: Dense layer\n",
    "    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(100, )))\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(256, kernel_size=(5, 5), strides=(1, 1), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "\n",
    "    # Fifth: Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(3, kernel_size=(5, 5), padding='same', activation='tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-relaxation",
   "metadata": {},
   "source": [
    "make_generator_model이라는 함수를 만들어서 언제든 생성자를 생성할 수 있도록 했다.   \n",
    "\n",
    "함수 내부에서는 먼저 tf.keras.Sequential()로 모델을 시작한 후 레이어를 차곡차곡 쌓아준다.   \n",
    "\n",
    "여기에서 가장 중요한 레이어는 바로 Conv2DTranspose 레이어다. Conv2DTranspose 층은 일반적인 Conv2D와 반대로 이미지 사이즈를 넓혀주는 층이다. 이 모델에서는 세 번의 Conv2DTranspose 층을 이용해 (8, 8, 256) → (16, 16, 64) → (32, 32, 1) 순으로 이미지를 키워나간다. 여기서 최종 사이즈인 (32, 32, 1)은 우리가 준비했던 데이터셋과 형상이 동일하다.\n",
    "\n",
    "참고: What is Transposed Convolutional Layer?\n",
    "레이어의 사이사이에 특정 층들이 반복되는 것을 확인할 수 있는데, BatchNormalization 레이어는 신경망의 가중치가 폭발하지 않도록 가중치 값을 정규화시켜준다. 또한 중간층들의 활성화 함수는 모두 LeakyReLU를 사용하였다. 다만 마지막 층에는 활성화 함수로 tanh를 사용하는데, 이는 우리가 -1 ~ 1 이내의 값으로 픽셀값을 정규화시켰던 데이터셋과 동일하게 하기 위함이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그럼 생성 모델을 generator라는 변수로 생성하고, 모델 세부 내용인 summary를 출력해 보자\n",
    "generator = make_generator_model()\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-recycling",
   "metadata": {},
   "source": [
    "그럼 모델이 만들어졌으니, shape=(1, 100)의 형상을 가지는 랜덤 노이즈 벡터를 생성해서 결과물을 한번 만들어 보도록 하겠습니다.   \n",
    "아직 모델이 학습되지 않았으니, 아마 결과물도 큰 의미가 있지는 않을 것이다.\n",
    "\n",
    "tf.random.normal을 이용하면 가우시안 분포에서 뽑아낸 랜덤 벡터로 이루어진 노이즈 벡터를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-radiation",
   "metadata": {},
   "source": [
    "텐서플로우 2.0 버전에서는 레이어와 모델에 call 메소드를 구현해 놓기 때문에, 방금 만들어진 생성자 모델에 입력값으로 노이즈를 넣고 바로 모델을 호출하면 간단히 결과 이미지가 생성된다 (내부적으로는 생성자의 call 함수가 호출된다).\n",
    "\n",
    "단, 지금은 학습하는 중이 아니니 training=False를 설정해 주어야 한다! Batch Normalization 레이어는 훈련 시기와 추론(infernce) 시기의 행동이 다르기 때문에 training=False을 주어야 올바른 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = generator(noise, training=False)\n",
    "generated_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-vietnam",
   "metadata": {},
   "source": [
    "[1, 32, 32, 32] 사이즈의 이미지가 잘 생성되었다!</br>   \n",
    "첫 번째 1은 1개(batch_size=1)라는 뜻을, 그 뒤로는 (32, 32, 1) 사이즈 이미지가 생성되었다는 뜻을 가진다.\n",
    "\n",
    "그러면 이 이미지를 또 시각화해서 봐야겠다.  \n",
    "matplotlib 라이브러리는 2차원 이미지만 보여줄 수 있으므로 0번째와 3번째 축의 인덱스를 0으로 설정해서 (32, 32) shape의 이미지를 꺼낼 수 있도록 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-intent",
   "metadata": {},
   "source": [
    "역시 -1과 1 사이의 값에서 적당히 잘 생성된 것을 확인할 수 있다.   \n",
    "아직은 모델이 전혀 학습하지 않은 상태이기 때문에 아무런 의미가 없는 노이즈 같은 이미지가 생성되었지만, 모델이 점차 학습해 나가며 제대로 된 이미지를 생성하기를 지켜보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-jacob",
   "metadata": {},
   "source": [
    "## 12.3 판별자 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-prince",
   "metadata": {},
   "source": [
    "그러면 이제 판별자를 설계할것이다!\n",
    "\n",
    "판별자는 앞서 알아봤듯 가짜 이미지와 진짜 이미지를 입력받으면 각 이미지 별로 '진짜라고 판단하는 정도'값을 출력해야 한다.\n",
    "그렇다면 입력은 (28, 28, 1) 크기의 이미지가, 출력은 단 하나의 숫자(진짜라고 판단하는 정도)가 될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 판별자 모델 정의\n",
    "def make_discriminator_model():\n",
    "\n",
    "    # Start\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # First: Conv2D Layer\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(1, 1), padding='same', input_shape=[32, 32, 3]))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Second: Conv2D Layer\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # Third: Flatten Layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Fourth: Dense Layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-northwest",
   "metadata": {},
   "source": [
    "판별자 또한 make_discriminator_model 함수로 구현하였다.\n",
    "\n",
    "Conv2DTranspose 층을 사용해서 이미지를 키워나갔던 생성자와 반대로, 판별자는 Conv2D 층으로 이미지의 크기를 점점 줄여나간다.\n",
    "첫 번째 Conv2D 층에서 입력된 [32, 32, 1] 사이즈의 이미지는 다음 층을 거치며 (32, 32, 1) → (16, 16, 64) → (8, 8, 128)까지 줄어들게 된다.\n",
    "\n",
    "마지막에는 Flatten 층을 사용해 3차원 이미지를 1차원으로 쭉 펴서 7x7x128=6272, 즉 (1, 6272) 형상의 벡터로 변환한다. 이는 생성자의 Reshape 층에서 1차원 벡터를 3차원으로 변환했던 것과 정확히 반대 역할을 한다. 1차원 벡터로 변환한 후에는 마지막 Dense Layer를 거쳐 단 하나의 값을 출력하게 된다.\n",
    "\n",
    "그럼 판별 모델을 discriminator라는 변수 이름으로 생성하고, 모델 세부 내용인 summary를 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그렇다면 아까 생성했던 가짜 이미지를 판별자에 입력시키면 어떤 결과가 나올까...바로 확인해 보자\n",
    "decision = discriminator(generated_image, training=False)\n",
    "decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-aruba",
   "metadata": {},
   "source": [
    "## 12.4 손실함수 최적화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-beauty",
   "metadata": {},
   "source": [
    "Real Image에 대한 라벨을 1, Fake Image에 대한 라벨을 0으로 두었을 때, 각각의 손실함수를 이용해 정량적으로 달성해야 하는 목표하는 결과는 다음과 같다.\n",
    "\n",
    "생성자 : 판별자가 Fake Image에 대해 판별한 값, 즉 D(fake_image) 값이 1에 가까워지는 것\n",
    "판별자 : Real Image 판별값, 즉 D(real_image)는 1에, Fake Image 판별값, 즉 D(fake_image)는 0에 가까워지는 것\n",
    "\n",
    "결국 생성자든 구분자든, 결국 손실함수에 들어가는 값은 모두 판별자의 판별값이 된다! 이러한 기준을 가지고 생성자, 판별자 각각에 대한 손실함수를 설계해 보자.\n",
    "\n",
    "손실함수에 사용할 교차 엔트로피 함수는 tf.keras.losses 라이브러리 안에 있다.\n",
    "\n",
    "다만, 우리가 교차 엔트로피를 계산하기 위해 입력할 값은 판별자가 판별한 값인데, 판별자 모델의 맨 마지막 Layer에는 값을 정규화시키는 sigmoid나 tanh 함수와 같은 활성화 함수가 없었다. 즉, 구분자가 출력하는 값은 범위가 정해지지 않아 모든 실숫값을 가질 수 있다.\n",
    "\n",
    "그런데 tf.keras.losses의 BinaryCrossEntropy 클래스는 기본적으로 본인에게 들어오는 인풋값이 0-1 사이에 분포하는 확률값이라고 가정한다. 따라서 from_logits를 True로 설정해 주어야 BinaryCrossEntropy에 입력된 값을 함수 내부에서 sigmoid 함수를 사용해 0~1 사이의 값으로 정규화한 후 알맞게 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-tattoo",
   "metadata": {},
   "source": [
    "cross_entropy를 활용해 계산할 loss들은 fake_output와 real_output, 두 가지를 활용한다.\n",
    "\n",
    "fake_output : 생성자가 생성한 Fake Image를 구분자에 입력시켜서 판별된 값, 즉 D(fake_image)   \n",
    "real_output : 기존에 있던 Real Image를 구분자에 입력시켜서 판별된 값, 즉 D(real_image)\n",
    "\n",
    "이제, fake_output과 real_output을 각각 1 또는 0에 비교를 해야 하는데, 바로 tf.ones_like()와 tf.zeros_like() 함수를 활용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떻게 사용되는지 보자\n",
    "vector = [[1, 2, 3],\n",
    "          [4, 5, 6]]\n",
    "\n",
    "tf.ones_like(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-disposition",
   "metadata": {},
   "source": [
    "generator_loss는 fake_output가 1에 가까워지기를 바라므로, 다음과 같이 tf.ones_like와의 교차 엔트로피값을 계산하면 된다.   \n",
    "즉, cross_entropy(tf.ones_like(fake_output), fake_output) 값은 fake_output이 (Real Image를 의미하는) 1에 가까울수록 작은 값을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-third",
   "metadata": {},
   "source": [
    "discriminator_loss\n",
    "반면, discriminator_loss는 real_output 값은 1에 가까워지기를, fake_output 값은 0에 가까워지기를 바라므로, 두 가지 loss값을 모두 계산한다. real_output은 1로 채워진 벡터와, fake_output은 0으로 채워진 벡터와 비교하면 된다!\n",
    "\n",
    "최종 discriminator_loss 값은 이 둘을 더한 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-thanksgiving",
   "metadata": {},
   "source": [
    "discriminator accuracy\n",
    "한편, 판별자가 real output, fake output을 얼마나 정확히 판별하는지의 accuracy를 계산해 보는 것도 GAN의 학습에서 매우 중요하다. 특히 두 accuracy를 따로 계산해서 비교해 보는 것이 매우 유용하다.\n",
    "\n",
    "만약 판별자가 real output과 fake output을 accuracy가 1.0에 가까울 정도로 정확하게 판별해 낸다면 좋은 것일까? 그렇지 않다. 이 경우 생성자가 만들어내는 fake output이 real output과 차이가 많이 나기 때문에 판별자가 매우 쉽게 판별해 내고 있다는 뜻이다. 그래서, real accuracy와 fake accuracy는 초반에는 1.0에 가깝게 나오다가, 서서히 낮아져서 둘 다 0.5에 가까워지는 것이 이상적이다. fake accuracy가 1.0에 더 가깝다면 아직은 생성자가 판별자를 충분히 잘 속이지 못하고 있다는 뜻이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_accuracy(real_output, fake_output):\n",
    "    real_accuracy = tf.reduce_mean(tf.cast(tf.math.greater_equal(real_output, tf.constant([0.5])), tf.float32))\n",
    "    fake_accuracy = tf.reduce_mean(tf.cast(tf.math.less(fake_output, tf.constant([0.5])), tf.float32))\n",
    "    return real_accuracy, fake_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-priest",
   "metadata": {},
   "source": [
    "손실함수는 모두 설계가 되었고, 이제 최적화 함수를 설정할 차례다. 이번에는 Adam 최적화 기법 을 활용해 보겠다.   \n",
    "중요한 점 한 가지는 생성자와 구분자는 따로따로 학습을 진행하는 개별 네트워크이기 때문에 optimizer를 따로 만들어주어야 한다는 점이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "generator_optimizer = tf.keras.optimizers.Adam(lr=0.0004, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(lr=0.0003, beta_1=0.5)\n",
    "\n",
    "# 학습 횟수 및 저장간격 설정\n",
    "save_every = 5\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-receiver",
   "metadata": {},
   "source": [
    "또한, 매번 학습이 어떻게 진행되어가고 있는지를 확인하기 위해 생성자가 생성한 샘플을 확인할 것이다.\n",
    "\n",
    "샘플은 한 번에 16장을 생성하도록 하겠다.\n",
    "생성할 샘플은 매번 같은 노이즈로 생성해야 그에 대한 진전 과정을 확인할 수 있으므로, 고정된 seed 노이즈를 만들어두어야 한다.\n",
    "즉, 100차원의 노이즈를 총 16개, (16, 100) 형상의 벡터를 만들어 두도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "seed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-burner",
   "metadata": {},
   "source": [
    "## 12.5 훈련과정 상세 기능 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 미니배치 당 진행할 train_step 함수를 먼저 만들어야 한다\n",
    "@tf.function\n",
    "def train_step(images):  #(1) 입력데이터\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])  #(2) 생성자 입력 노이즈\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:  #(3) tf.GradientTape() 오픈\n",
    "        generated_images = generator(noise, training=True)  #(4) generated_images 생성\n",
    "\n",
    "        #(5) discriminator 판별\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        #(6) loss 계산\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        #(7) accuracy 계산\n",
    "        real_accuracy, fake_accuracy = discriminator_accuracy(real_output, fake_output) \n",
    "    \n",
    "    #(8) gradient 계산\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    #(9) 모델 학습\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, real_accuracy, fake_accuracy  #(10) 리턴값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-sixth",
   "metadata": {},
   "source": [
    "그러면, 이렇게 한 단계씩 학습할 train_step과 함께 일정 간격으로 학습 현황을 볼 수 있는 샘플을 생성하는 함수를 만들어 보자.   \n",
    "아까 만들어 놓았던 고정된 seed를 이용해서 결과물을 만들어내므로, 고정된 seed에 대한 결과물이 얼마나 나아지고 있는지를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, it, sample_seeds):\n",
    "\n",
    "    predictions = model(sample_seeds, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow((predictions[i] + 1)/2.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('{}/aiffel/dcgan_newimage/cifar3/generated_samples/sample_epoch_{:04d}_iter_{:03d}.png'\n",
    "                    .format(os.getenv('HOME'), epoch, it))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-video",
   "metadata": {},
   "source": [
    "학습 과정을 체크하기 위해 시각화해 보아야 할 중요한 것으로 loss와 accuracy 그래프를 빼놓을 수 없다. GAN의 학습 과정은 지도학습 모델보다 까다로운데, 이것은 두 모델이 서로의 학습 과정에 영향을 주고받기 때문이다. train_step() 함수가 리턴하는 gen_loss, disc_loss, real_accuracy, fake_accuracy 이상 4가지 값을 history라는 dict 구조에 리스트로 저장하고 있다가 매 epoch마다 시각화하는 함수를 만들어 보자. 예를 들어 생성자의 loss의 history는 history['gen_loss']로 접근할 수 있는 list로 관리할 예정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6    # matlab 차트의 기본 크기를 15,6으로 지정해 줍니다.\n",
    "\n",
    "def draw_train_history(history, epoch):\n",
    "    # summarize history for loss  \n",
    "    plt.subplot(211)  \n",
    "    plt.plot(history['gen_loss'])  \n",
    "    plt.plot(history['disc_loss'])  \n",
    "    plt.title('model loss')  \n",
    "    plt.ylabel('loss')  \n",
    "    plt.xlabel('batch iters')  \n",
    "    plt.legend(['gen_loss', 'disc_loss'], loc='upper left')  \n",
    "\n",
    "    # summarize history for accuracy  \n",
    "    plt.subplot(212)  \n",
    "    plt.plot(history['fake_accuracy'])  \n",
    "    plt.plot(history['real_accuracy'])  \n",
    "    plt.title('discriminator accuracy')  \n",
    "    plt.ylabel('accuracy')  \n",
    "    plt.xlabel('batch iters')  \n",
    "    plt.legend(['fake_accuracy', 'real_accuracy'], loc='upper left')  \n",
    "    \n",
    "    # training_history 디렉토리에 epoch별로 그래프를 이미지 파일로 저장합니다.\n",
    "    plt.savefig('{}/aiffel/dcgan_newimage/cifar3/training_history/train_history_{:04d}.png'\n",
    "                    .format(os.getenv('HOME'), epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정기적으로 모델을 저장하기 위한 checkpoint를 만들어준다\n",
    "checkpoint_dir = os.getenv('HOME')+'/aiffel/dcgan_newimage/cifar3/training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-contemporary",
   "metadata": {},
   "source": [
    "## 12.6 학습 과정 진행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-legend",
   "metadata": {},
   "source": [
    "지금까지 한 단계를 학습하는 train_step, 샘플 이미지를 생성하고 저장하기 위한 generate_and_save_images(), 학습 과정을 시각화하는 draw_train_history(), 그리고 모델까지 저장하기 위한 checkpoint까지 모두 준비가 되었으니 이것들을 한 곳에 합쳐보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, save_every):\n",
    "    start = time.time()\n",
    "    history = {'gen_loss':[], 'disc_loss':[], 'real_accuracy':[], 'fake_accuracy':[]}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        for it, image_batch in enumerate(dataset):\n",
    "            gen_loss, disc_loss, real_accuracy, fake_accuracy = train_step(image_batch)\n",
    "            history['gen_loss'].append(gen_loss)\n",
    "            history['disc_loss'].append(disc_loss)\n",
    "            history['real_accuracy'].append(real_accuracy)\n",
    "            history['fake_accuracy'].append(fake_accuracy)\n",
    "\n",
    "            if it % 50 == 0:\n",
    "                display.clear_output(wait=True)\n",
    "                generate_and_save_images(generator, epoch+1, it+1, seed)\n",
    "                print('Epoch {} | iter {}'.format(epoch+1, it+1))\n",
    "                print('Time for epoch {} : {} sec'.format(epoch+1, int(time.time()-epoch_start)))\n",
    "\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epochs, it, seed)\n",
    "        print('Time for training : {} sec'.format(int(time.time()-start)))\n",
    "\n",
    "        draw_train_history(history, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_every = 5\n",
    "EPOCHS = 100\n",
    "\n",
    "# 사용가능한 GPU 디바이스 확인\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS, save_every)\n",
    "\n",
    "# 학습과정의 loss, accuracy 그래프 이미지 파일이 ~/aiffel/dcgan_newimage/cifar10/training_history 경로에 생성되고 있으니\n",
    "# 진행 과정을 수시로 확인해 보는걸 권장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "numerical-support",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-96a6a6dff051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "# 학습과정 시각화하기\n",
    "anim_file = os.getenv('HOME')+'/aiffel/dcgan_newimage/cifar3/Animals.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('{}/aiffel/dcgan_newimage/cifar3/generated_samples/sample*.png'.format(os.getenv('HOME')))\n",
    "    filenames = sorted(filenames)\n",
    "    last = -1\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frame = 2*(i**0.5)\n",
    "        if round(frame) > round(last):\n",
    "            last = frame\n",
    "        else:\n",
    "            continue\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "\n",
    "!ls -l ~/aiffel/dcgan_newimage/cifar3/Animals.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-involvement",
   "metadata": {},
   "source": [
    "## 12.7 GAN 훈련 과정 개선하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(15):\n",
    "    noise = tf.random.normal([1, 100])\n",
    "    generated_image = generator(noise, training=False)\n",
    "    np_generated = generated_image.numpy()\n",
    "    np_generated = (np_generated * 127.5) + 127.5   # reverse of normalization\n",
    "    np_generated = np_generated.astype(int)\n",
    "\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np_generated[0])\n",
    "    \n",
    "plt.show()  # 정상적으로 모델이 로드되었다면 랜덤 이미지가 아니라 CIFAR-10 이미지가 그려질 것이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 학습한 결과를 출력해 보니 원인 모를 형체가 나왔다... 여기서 걔선을 시키기 위해 몇 가지 파라미터를 바꿔서 훈련을 진행하겠다\n",
    "# https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-cifar-10-small-object-photographs-from-scratch/\n",
    "# 위의 링크를 참고해서 진행하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-tutorial",
   "metadata": {},
   "source": [
    "## 12.8 회고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-smooth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
