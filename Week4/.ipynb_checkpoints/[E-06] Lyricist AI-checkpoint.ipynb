{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intended-metallic",
   "metadata": {},
   "source": [
    "# 6. 작사가 AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-concrete",
   "metadata": {},
   "source": [
    "## 6.1 데이터를 불러오고 확인해본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beautiful-copying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['', '\\t\\t\\t“There must be some way out of here,” said the joker to the thief', '“There’s too much confusion, I can’t get no relief']\n"
     ]
    }
   ],
   "source": [
    "# 먼저 프로젝트를 진행하기 전에 아래와 같은 작업이 필요하다\n",
    "# $ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "# $ unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics\n",
    "\n",
    "# 데이터를 읽어온다\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담는다\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "olive-paradise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t“There must be some way out of here,” said the joker to the thief\n",
      "“There’s too much confusion, I can’t get no relief\n",
      "Businessmen, they drink my wine, plowmen dig my earth\n",
      "None of them along the line know what any of it is worth”\n",
      "“No reason to get excited,” the thief, he kindly spoke\n",
      "“There are many here among us who feel that life is but a joke\n",
      "But you and I, we’ve been through that, and this is not our fate\n",
      "So let us not talk falsely now, the hour is getting late”\n"
     ]
    }
   ],
   "source": [
    "# 문장이 어떤 것이 있는지 확인해보자\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뛴다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뛴다.\n",
    "\n",
    "    if idx > 10: break   # 문장 10개를 확인해보자\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-peter",
   "metadata": {},
   "source": [
    "## 6.2 데이터를 전처리한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spectacular-carnival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화하기 전에 정규표현식을 이용해서 문장을 필터링 해주는 함수를 만든다\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀐다\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 준다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 문장이 어떻게 필터링 됐는지 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "communist-enough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> there s too much confusion , i can t get no relief <end>',\n",
       " '<start> businessmen , they drink my wine , plowmen dig my earth <end>',\n",
       " '<start> no reason to get excited , the thief , he kindly spoke <end>',\n",
       " '<start> all along the watchtower , princes kept the view <end>',\n",
       " '<start> while all the women came and went , barefoot servants , too <end>',\n",
       " '<start> outside in the distance a wildcat did growl <end>',\n",
       " '<start> two riders were approaching , the wind began to howl <end>',\n",
       " '<start> once upon a time you dressed so fine <end>',\n",
       " '<start> you threw the bums a dime in your prime , didn t you ? <end>',\n",
       " '<start> people d call , say , beware doll , you re bound to fall <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제 데이터를 완성한다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence.split()) >=12 : continue\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뛴다\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "basic-twins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   62   16 ...    0    0    0]\n",
      " [   2    1    4 ...    0    0    0]\n",
      " [   2   37  595 ...    0    0    0]\n",
      " ...\n",
      " [   2    9  156 ...    0    0    0]\n",
      " [   2    8    9 ...    0    0    0]\n",
      " [   2    6 3227 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f79bb71b3d0>\n"
     ]
    }
   ],
   "source": [
    "# 정제된 데이터를 토큰화하고, 단어 사전을 만들고, 데이터를 숫자로 변환한다.\n",
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12500,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있지만, 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 된다\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축한다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환한다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공한다\n",
    "    # maxlen의 디폴트값은 None이다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰진다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "great-rental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   62   16  101  186 2633    4    5   33   15]\n",
      " [   2    1    4   43  488   13  986    4    1 1037]\n",
      " [   2   37  595   10   45 2491    4    6 2237    4]]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서 데이터를 3번째 행부터 10번째 열까지 출력해 보자\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sophisticated-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : t\n"
     ]
    }
   ],
   "source": [
    "# 텐서 데이터는 모두 정수로 이우러져 있는데, 이것은 tokenizer에 구축된 단어 사전의 인덱스다. 단어 사전이 어떻게 구축되었는지 확인을 한번 해보자\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 15: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "missing-stroke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   62   16  101  186 2633    4    5   33   15   45   37 5623    3\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n",
      "[  62   16  101  186 2633    4    5   33   15   45   37 5623    3    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 패딩 처리\n",
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성한다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성한다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dominican-hollywood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((450, 29), (450, 29)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 객체를 생성한다.\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 450\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 10000개와, 여기 포함되지 않은 0:<pad>를 포함하여 10001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "remarkable-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 적절하게 나눠준다\n",
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "civilian-excitement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (121675, 29)\n",
      "Target Train: (121675, 29)\n"
     ]
    }
   ],
   "source": [
    "# 잘 나눠졌는지 확인해 보자\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-angel",
   "metadata": {},
   "source": [
    "# 6.3 모델을 생성하고 평가해본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fiscal-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 -> 1개의 Embedding 레이어와 2개의 LSTM 레이어 그리고 1개의 Dense 레이어로 구성한다\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smooth-order",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(450, 29, 12501), dtype=float32, numpy=\n",
       "array([[[ 3.8185174e-04, -1.7984827e-05, -1.6679820e-04, ...,\n",
       "         -1.3133374e-05,  1.7482019e-04, -1.0425178e-04],\n",
       "        [ 4.2313605e-04, -3.5183923e-04, -2.5149572e-04, ...,\n",
       "         -3.8682108e-04,  7.4056923e-05, -2.7054608e-05],\n",
       "        [ 6.4476411e-04, -5.6856166e-04, -3.8029355e-04, ...,\n",
       "         -7.2526408e-04,  9.5069270e-05, -1.8655547e-06],\n",
       "        ...,\n",
       "        [ 1.7740838e-05,  4.8608781e-04,  8.1505073e-04, ...,\n",
       "          4.7086687e-03,  3.1954604e-03,  1.4046150e-04],\n",
       "        [ 7.2095943e-05,  4.5076141e-04,  9.1457617e-04, ...,\n",
       "          4.8398762e-03,  3.2978568e-03,  1.6746762e-04],\n",
       "        [ 1.2260905e-04,  4.1703961e-04,  9.9909690e-04, ...,\n",
       "          4.9491972e-03,  3.3874011e-03,  1.9553170e-04]],\n",
       "\n",
       "       [[ 3.8185174e-04, -1.7984827e-05, -1.6679820e-04, ...,\n",
       "         -1.3133374e-05,  1.7482019e-04, -1.0425178e-04],\n",
       "        [ 8.8547991e-04, -2.9163528e-04, -4.4288874e-05, ...,\n",
       "         -2.6534937e-04,  2.3349628e-04,  5.9054088e-05],\n",
       "        [ 9.7531470e-04, -4.5285781e-04,  7.4078547e-05, ...,\n",
       "         -7.6206365e-05,  4.9733405e-04,  5.5872482e-05],\n",
       "        ...,\n",
       "        [ 2.8512909e-04,  2.9074372e-04,  1.0541803e-03, ...,\n",
       "          5.1482795e-03,  3.4307262e-03,  1.8007231e-04],\n",
       "        [ 3.1334726e-04,  2.7152753e-04,  1.1131089e-03, ...,\n",
       "          5.2084462e-03,  3.4936299e-03,  2.1024697e-04],\n",
       "        [ 3.3936455e-04,  2.5398142e-04,  1.1628151e-03, ...,\n",
       "          5.2567269e-03,  3.5497567e-03,  2.3973835e-04]],\n",
       "\n",
       "       [[ 3.8185174e-04, -1.7984827e-05, -1.6679820e-04, ...,\n",
       "         -1.3133374e-05,  1.7482019e-04, -1.0425178e-04],\n",
       "        [ 5.0149724e-04,  5.9349713e-05, -3.4916616e-04, ...,\n",
       "          9.3206894e-05,  3.6871593e-04, -1.0596638e-04],\n",
       "        [ 6.2544498e-04,  1.8010386e-04, -1.8527813e-04, ...,\n",
       "          1.0656427e-04,  4.5050756e-04, -2.0363903e-05],\n",
       "        ...,\n",
       "        [ 2.8937834e-04,  3.4294737e-04,  1.1094471e-03, ...,\n",
       "          5.0577410e-03,  3.4063878e-03,  1.7382158e-04],\n",
       "        [ 3.1909434e-04,  3.2181817e-04,  1.1572872e-03, ...,\n",
       "          5.1273173e-03,  3.4780588e-03,  1.9648693e-04],\n",
       "        [ 3.4564393e-04,  3.0169846e-04,  1.1976542e-03, ...,\n",
       "          5.1844209e-03,  3.5408102e-03,  2.2061702e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.8185174e-04, -1.7984827e-05, -1.6679820e-04, ...,\n",
       "         -1.3133374e-05,  1.7482019e-04, -1.0425178e-04],\n",
       "        [ 7.1876548e-04,  8.4147126e-05, -1.4336107e-04, ...,\n",
       "         -3.1248410e-04,  1.8299940e-04, -3.2566089e-04],\n",
       "        [ 1.0980155e-03,  5.2826574e-05, -1.0930135e-04, ...,\n",
       "         -3.7112809e-04,  3.6834048e-05, -5.8621587e-04],\n",
       "        ...,\n",
       "        [ 1.6646896e-04,  3.4483281e-04,  9.1436674e-04, ...,\n",
       "          5.0675645e-03,  3.2518266e-03,  2.0726598e-04],\n",
       "        [ 2.1192612e-04,  3.1970348e-04,  9.9271431e-04, ...,\n",
       "          5.1435293e-03,  3.3451689e-03,  2.2899771e-04],\n",
       "        [ 2.5327812e-04,  2.9711574e-04,  1.0592509e-03, ...,\n",
       "          5.2049058e-03,  3.4271025e-03,  2.5172255e-04]],\n",
       "\n",
       "       [[ 3.8185174e-04, -1.7984827e-05, -1.6679820e-04, ...,\n",
       "         -1.3133374e-05,  1.7482019e-04, -1.0425178e-04],\n",
       "        [ 4.9854058e-04,  9.6080648e-06, -4.4980610e-04, ...,\n",
       "         -2.7307452e-04,  3.3116387e-04, -2.8995547e-04],\n",
       "        [ 4.7271274e-04, -6.4106382e-05, -7.2782912e-04, ...,\n",
       "         -3.1540237e-04,  1.5351770e-04, -3.0139805e-04],\n",
       "        ...,\n",
       "        [ 3.5182733e-04,  2.8794899e-04,  1.0887017e-03, ...,\n",
       "          5.0657308e-03,  3.4688208e-03,  2.2257987e-04],\n",
       "        [ 3.7189375e-04,  2.7306267e-04,  1.1447433e-03, ...,\n",
       "          5.1359343e-03,  3.5342264e-03,  2.3952620e-04],\n",
       "        [ 3.9021208e-04,  2.5891195e-04,  1.1913794e-03, ...,\n",
       "          5.1929867e-03,  3.5908723e-03,  2.5850814e-04]],\n",
       "\n",
       "       [[ 3.8185174e-04, -1.7984827e-05, -1.6679820e-04, ...,\n",
       "         -1.3133374e-05,  1.7482019e-04, -1.0425178e-04],\n",
       "        [ 4.8647588e-04, -1.3857811e-04, -1.3074304e-04, ...,\n",
       "         -2.3441091e-04,  1.3378094e-04, -1.9753880e-04],\n",
       "        [ 3.8995876e-04,  3.0695592e-04, -4.0568670e-04, ...,\n",
       "         -2.2932352e-04,  2.6307465e-04, -1.2964731e-05],\n",
       "        ...,\n",
       "        [ 4.1465298e-04,  2.3390359e-04,  1.2778140e-03, ...,\n",
       "          5.3922050e-03,  3.6054840e-03,  2.1117814e-04],\n",
       "        [ 4.2963965e-04,  2.2319797e-04,  1.2992183e-03, ...,\n",
       "          5.4091839e-03,  3.6473796e-03,  2.4647854e-04],\n",
       "        [ 4.4307124e-04,  2.1353981e-04,  1.3166541e-03, ...,\n",
       "          5.4206336e-03,  3.6847421e-03,  2.7890512e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델의 shape을 살펴보자\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "activated-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3200256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12813525  \n",
      "=================================================================\n",
      "Total params: 29,653,461\n",
      "Trainable params: 29,653,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 만들어진 모델을 살펴본다\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "coastal-resolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-ebbdf77b3556>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU가 사용가능 한 상태인지 확인해본다\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spoken-three",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n",
      "Epoch 1/10\n",
      "337/337 [==============================] - 148s 438ms/step - loss: 1.8460 - accuracy: 0.7452\n",
      "Epoch 2/10\n",
      "337/337 [==============================] - 146s 434ms/step - loss: 1.4820 - accuracy: 0.7689\n",
      "Epoch 3/10\n",
      "337/337 [==============================] - 147s 436ms/step - loss: 1.4038 - accuracy: 0.7744\n",
      "Epoch 4/10\n",
      "337/337 [==============================] - 147s 437ms/step - loss: 1.3451 - accuracy: 0.7782\n",
      "Epoch 5/10\n",
      "337/337 [==============================] - 147s 437ms/step - loss: 1.2956 - accuracy: 0.7815\n",
      "Epoch 6/10\n",
      "337/337 [==============================] - 147s 437ms/step - loss: 1.2499 - accuracy: 0.7848\n",
      "Epoch 7/10\n",
      "337/337 [==============================] - 147s 435ms/step - loss: 1.2082 - accuracy: 0.7883\n",
      "Epoch 8/10\n",
      "337/337 [==============================] - 147s 435ms/step - loss: 1.1695 - accuracy: 0.7917\n",
      "Epoch 9/10\n",
      "337/337 [==============================] - 147s 435ms/step - loss: 1.1335 - accuracy: 0.7953\n",
      "Epoch 10/10\n",
      "337/337 [==============================] - 146s 435ms/step - loss: 1.1001 - accuracy: 0.7989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nhistory = model.fit(enc_train, \\n          dec_train, \\n          epochs=12,\\n          batch_size=400,\\n          validation_data=(enc_val, dec_val),\\n          verbose=1)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 시작 (에포크를 10번 정도 돌리려 했으니 시간 관계상 3번으로 끝낸다)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "# the save point\n",
    "checkpoint_dir = os.getenv('HOME')+'/aiffel/lyricist/models2/'\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 mode='auto',\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "print(\"✅\")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "history_lycist=model.fit(dataset, epochs=10)\n",
    "'''\n",
    "history = model.fit(enc_train, \n",
    "          dec_train, \n",
    "          epochs=12,\n",
    "          batch_size=400,\n",
    "          validation_data=(enc_val, dec_val),\n",
    "          verbose=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-accordance",
   "metadata": {},
   "source": [
    "## 6.4 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hairy-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환한다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 됨\n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 모델이 예측한 마지막 단어가 새롭게 생성한 단어가 됨\n",
    "\n",
    "        # 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줌\n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 됨\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 최종적으로 모델이 생성한 자연어 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "american-french",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m so bad <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-request",
   "metadata": {},
   "source": [
    "## 회고\n",
    "한주가 정말로 빨리간다... 이번 프로젝트에서는 화요일에 진행한 프로젝트와 같은 카테고리에 있는 자연어처리를 진행했는데, 그때는 음성을, 지금은 글을 어떻게 딥러닝모델에 담고 학습을 시키는지를 대략적으로 알게 되었다. 화요일 노드와 달리 이번에는 개념이 어렵지 않았고 복습도 아주 많은 시간을 들이지는 않을 거 같다. 내가 미완성된 문장을 주면 모델이 완성된 문장을 다시 리턴해주는 것과 모델을 다시 만들고 다시 학습을 시키면 리턴된 문장이 매번 달라지는 것이 신기했다. 프로젝트를 진행하면서 딱히 어려운 점은 없었으나, 모델이 무거워서 학습시간이 오래걸렸다. 점심을 먹고 다시 모델을 돌리니 예상시간이 엄청나게 길게 잡혀서 학습을 중단하고 tf.test.is_gpu_available() 명령어로 gpu가 사용가능한 상태인지 확인했더니 flase가 떠서 노트북을 껐다가 켰다. 그랬더니 True가 뜨고 모델이 어느정도 이해가능한?? 수준의 학습속도로 돌아오더라...역시 에러를 해결하는 확실한 방법은 재부팅이 아닐까... 여튼, 루브릭에서 요구하는 모든 평가문항들을 잘 수행했으니 이번에도 3별을 받지 않을까. 아 그리고 노드에서 \"10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요!\"라고 되어 있는데 프로젝트에서 어딘가 꼬이지 않는 이상은 1 Epoch에 2.2 이하로 내려가는 것을 볼 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
